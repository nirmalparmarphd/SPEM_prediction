{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import seaborn as sns \n",
    "import matplotlib as plt\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data\n",
    "\n",
    "raw_data = pd.read_csv('six_blades.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data1 = raw_data.iloc[3:, :]\n",
    "raw_data1.columns = ['Date_April','Max_T','Min_T', 'RH(%)', 'ws', 'WS(mps)', 'rf','Den(Be)','Evp(mm)']\n",
    "raw_data2 = raw_data1.drop(columns=['ws','rf'])\n",
    "raw_data2.reset_index(drop=True, inplace=True)\n",
    "raw_data2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data\n",
    "data = raw_data2.iloc[:,0:-1]\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target data\n",
    "target = raw_data2.iloc[:,-1]\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data checkpoint\n",
    "input_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardlisation of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardlisation of data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_input = scaler.fit_transform(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(scaler, 'spem_scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.arange(scaled_input.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "shuffled_inputs = scaled_input[shuffled_indices]\n",
    "shuffled_targets = target[shuffled_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_count = shuffled_indices.shape[0]\n",
    "\n",
    "train_sample_count = int(0.8 * sample_count)\n",
    "validation_sample_count = int(0.1 * sample_count)\n",
    "test_sample_count = int(0.1 * sample_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = shuffled_inputs[:train_sample_count]\n",
    "train_targets = shuffled_targets[:train_sample_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_input = shuffled_inputs[train_sample_count:train_sample_count+validation_sample_count]\n",
    "validation_target = shuffled_targets[train_sample_count:train_sample_count+validation_sample_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = shuffled_inputs[train_sample_count+validation_sample_count:]\n",
    "test_target = shuffled_targets[train_sample_count+validation_sample_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(train_targets), train_sample_count, np.sum(train_targets) / train_sample_count)\n",
    "print(np.sum(validation_target), validation_sample_count, np.sum(validation_target) / validation_sample_count)\n",
    "print(np.sum(test_target), test_sample_count, np.sum(test_target) / test_sample_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save datasets in *.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('seawater_data_train', inputs=train_input, targets=train_targets)\n",
    "np.savez('seawater_data_validation', inputs=validation_input, targets=validation_target)\n",
    "np.savez('seawater_data_test', inputs=test_input, targets=test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ebd78eb58732ed51a1214ed250f845ad2ca7676699e3bdc8f074c0f0c51034e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
